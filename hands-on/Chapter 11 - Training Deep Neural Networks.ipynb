{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training deep neural networks can involve a number of problems, such as\n",
    "\n",
    "- gradients becoming too small to influence lower layers during backpropagation (vanishing)\n",
    "- gradients becoming too large to converge (exploding)\n",
    "- training data that's insufficient or impractical to label\n",
    "- slow training\n",
    "- too many parameters, not enough instances\n",
    "\n",
    "Each of these issues can be solved with the right techniques, enabling training of very deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing and Exploding Gradients\n",
    "\n",
    "Deep neural networks suffer from unstable gradients, where different layers can learn at very different speeds. During backpropagation, gradients tend to shrink as they progress towards the lower layers, sometimes to the point that the lower layer weights won't ever change. The opposite can also happen, resulting in very high weights that force the algorithm to diverge.\n",
    "\n",
    "This issue was made clear by the popular combination of two factors:\n",
    "\n",
    "1. logistic sigmoid activation function\n",
    "2. initialization following a normal distribution with $\\mu=0$ and $\\sigma=1$\n",
    "\n",
    "The sigmoid activation function saturates at 0 or 1 with a derivative near 0 for very large inputs, so by the time backpropagation begins it usually can't change much.\n",
    "\n",
    "The Glorot initialization was introduced to alleviate this issue, based on the assertion that the variance of the outputs of each layer be equal to the variance of its inputs and that the gradients have equal variance before and after flowing through a layer. A practical approximation of this was defined to randomly initialize the connection weights of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Equation 1: Glorot initialization for use with the logistic activation function*\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Normal distribution with } \\mu=0 \\text{ and } \\sigma^2=\\frac{1}{{fan}_{avg}}\\\\[3ex]\n",
    "\\text{or uniform distribution between } -r \\text{ and } +r\\text{, with }r = \\sqrt{\\frac{3}{{fan}_{avg}}}\n",
    "\\end{equation*}\n",
    "\n",
    "where\n",
    "\n",
    "- ${fan}_{avg} = \\frac{fan_{in} + fan_{out}}{2}$\n",
    "- ${fan}_{in}$ is the number of inputs in a layer\n",
    "- ${fan}_{out}$ is the number of neurons in a layer\n",
    "\n",
    "A similar strategy is the LeCun initialization, which replaces ${fan}_{avg}$ with ${fan}_{in}$, which is useful when paired with the SELU activation function. Similarly, He initialization uses $\\sigma^2 = \\frac{2}{{fan}_{in}}$ to be more useful alongside ReLU and its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
