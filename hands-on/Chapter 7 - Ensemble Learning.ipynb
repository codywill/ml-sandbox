{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ensemble Learning* involves training a group of predictors (an *ensemble*) and aggregating their predictions to achieve better accuracy than any of the individual models.\n",
    "\n",
    "Ensemble methods are typically applied near the end of a project after some effective predictors have been identified.\n",
    "\n",
    "Topics:\n",
    "- Voting Classifiers\n",
    "- Bagging & Pasting\n",
    "- Random Forests\n",
    "- Boosting\n",
    "- Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifiers\n",
    "\n",
    "*Voting classifiers* aggregate the predictions of different classifiers.\n",
    "\n",
    "- *Hard voting*: each classifier makes a prediction and the class with the most votes is selected\n",
    "- *Soft voting*: each classifier determines the probability of each class and the class with the highest average probability is selected\n",
    "\n",
    "An ensemble's accuracy tends to improve when the classifiers are more independent from one another. Different algorithms are subject to different types of errors, so a diverse group is less likely to repeat the same errors and thus ensures the ensemble will be more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Demonstration using the moons dataset\n",
    "X, y = make_moons(n_samples=500, noise=0.30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     cr...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=None,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=True, random_state=None,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "\n",
    "hard_voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "hard_voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: 0.864\n",
      "RandomForestClassifier: 0.92\n",
      "SVC: 0.904\n",
      "VotingClassifier: 0.896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracies(clf_tuple):\n",
    "    # Check the accuracy of each model\n",
    "    for clf in clf_tuple:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(f\"{clf.__class__.__name__}: {accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "accuracies((log_clf, rnd_clf, svm_clf, hard_voting_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     cr...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=None,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=True, random_state=None,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Soft voting increases training time\n",
    "soft_voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "soft_voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: 0.864\n",
      "RandomForestClassifier: 0.912\n",
      "SVC: 0.904\n",
      "VotingClassifier: 0.912\n"
     ]
    }
   ],
   "source": [
    "accuracies((log_clf, rnd_clf, svm_clf, soft_voting_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging & Pasting\n",
    "\n",
    "Another ensemble learning approach is to use the same training algorithmm for each predictor but train them on different random subsets of the training data. *Bagging* (or bootstrap aggregating) and *pasting* are two sampling methods for this approach.\n",
    "\n",
    "- *Bagging*: sampling with replacement\n",
    "- *Pasting*: sampling without replacement\n",
    "\n",
    "Predictions are made via aggregation, typically using the statistical mode function (most frequent prediction) similar to a hard voting classifier. Each predictor will have a higher bias due to the smaller sample of training data, but the ensemble benefits from this much like it does from error diversity in voting classification, resulting in a lower overall bias and variance.\n",
    "\n",
    "Bagging and pasting can train and predict in parallel, so they are attractive methods for scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        presort='deprecated',\n",
       "                                                        random_state=None,\n",
       "                                                        splitter='best'),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=100, n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "                  random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 500 decision trees with 100 samples each, using maximum number of CPU cores\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), \n",
    "    n_estimators=500, \n",
    "    max_samples=100,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier: 0.92\n"
     ]
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "print(f\"BaggingClassifier: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        presort='deprecated',\n",
       "                                                        random_state=None,\n",
       "                                                        splitter='best'),\n",
       "                  bootstrap=False, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=100, n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "                  random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable bootstrap for pasting\n",
    "past_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), \n",
    "    n_estimators=500, \n",
    "    max_samples=100,\n",
    "    bootstrap=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "past_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PastingClassifier: 0.92\n"
     ]
    }
   ],
   "source": [
    "y_pred = past_clf.predict(X_test)\n",
    "print(f\"PastingClassifier: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using replacement (bagging), each predictor samples ~63% of the training data. The remaining ~37% is considered *out-of-bag* (oob). This data can be used to evaluate the classifier after training and predict how accurate it will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), \n",
    "    n_estimators=500, \n",
    "    max_samples=100,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with accuracy using the test set\n",
    "accuracy_score(y_test, bag_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90625   , 0.09375   ],\n",
       "       [0.67639257, 0.32360743],\n",
       "       [0.00516796, 0.99483204],\n",
       "       [0.0984456 , 0.9015544 ],\n",
       "       [0.952     , 0.048     ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access probabilities of each classifier in the bag\n",
    "bag_clf.oob_decision_function_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictors can also be trained on a random subset of the input features.\n",
    "\n",
    "- *Random Patches*: sampling both instances and features\n",
    "- *Random Subspaces*: keeping all instances and sampling only features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8693333333333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random patches\n",
    "bag_clf_patch = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), \n",
    "    n_estimators=500, \n",
    "    max_samples=100,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True,\n",
    "    bootstrap_features=True,\n",
    "    max_features=0.5\n",
    ")\n",
    "bag_clf_patch.fit(X_train, y_train)\n",
    "bag_clf_patch.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.832"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, bag_clf_patch.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.664"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random subspaces\n",
    "bag_clf_sub = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), \n",
    "    n_estimators=500, \n",
    "    max_samples=1.0,\n",
    "    bootstrap=False,\n",
    "    n_jobs=-1,\n",
    "    bootstrap_features=True,\n",
    "    max_features=0.5\n",
    ")\n",
    "bag_clf_sub.fit(X_train, y_train)\n",
    "accuracy_score(y_test, bag_clf_sub.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random patches and subspaces are mostly useful when handling high-dimensional inputs such as images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "A *Random Forest* is an ensemble of decision trees typically trained via bagging with max samples hyperparameter set to the size of the training set.\n",
    "\n",
    "The RandomForestClassifier is an optimized alternative to creating a BaggingClassifier with DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=16, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests can be made even more random by using random feature thresholds when growing trees instead of searching for the best possible thresholds. These trees are called *Extremely Randomized Trees*, or *Extra-Trees*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                     criterion='gini', max_depth=None, max_features='auto',\n",
       "                     max_leaf_nodes=16, max_samples=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=1, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "                     oob_score=False, random_state=None, verbose=0,\n",
       "                     warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "et_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_et = et_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests are useful for measuring feature importance by observing how nodes using each feature reduce impurity across all trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm): 0.09217234903002987\n",
      "sepal width (cm): 0.025003091206376275\n",
      "petal length (cm): 0.4473647116683714\n",
      "petal width (cm): 0.43545984809522237\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(f'{name}: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "*Boosting* refers to the method of combining several weak learners into a strong learner, usually trained sequentially. *AdaBoost* (or *Adaptive Boosting*) and *Gradient Boosting* are two very common methods.\n",
    "\n",
    "Since training is done sequentially, this method can't be parallelized and therefore doesn't scale as well as bagging or pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "The *AdaBoost* method start by training a base classifier, increases the weights for misclassified instances, and trains another classifier with the updated weights. This process is repeated for all predictors in the ensemble, and predictions are made via aggregation as with bagging or pasting with weights assigned according to the accuracy of the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Determine the error rate of a predictor\n",
    "\n",
    "*Equation 1: Weighted error rate of the $j^{th}$ predictor*\n",
    "\n",
    "\\begin{equation*}\n",
    "r_j = \\frac{\n",
    "    \\sum_{\\substack{\n",
    "        \\hat{y}_j^{(i)}\n",
    "        \\neq\n",
    "        y^{(i)}\n",
    "    \\\\i=1}}^m w^{(i)}\n",
    "}{\n",
    "    \\sum_{i=1}^m w^{(i)}\n",
    "}\n",
    "\\end{equation*}\n",
    "\n",
    "- $\\hat{y}_j^{(i)}$ is the $j^{th}$ predictor's prediction for the $i^{th}$ instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Determine the predictor's weight\n",
    "\n",
    "*Equation 2: Predictor weight*\n",
    "\n",
    "\\begin{equation*}\n",
    "\\alpha_j = \\eta \\text{log} \\frac{1-r_j}{r_j}\n",
    "\\end{equation*}\n",
    "\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\alpha_j \\gt 0$ when the predictor is $\\gt 50$% accurate\n",
    "- $\\alpha_j \\lt 0$ when the predictor is $\\lt 50$% accurate\n",
    "- $\\alpha_j \\approx 0$ when the predictor is $\\approx 50$% accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Boost misclassified instance weights\n",
    "\n",
    "*Equation 3: Weight update rule*\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{for } i=1, 2, \\cdots, m\\\\\n",
    "w^{(i)} \\leftarrow\n",
    "    \\begin{cases}\n",
    "    & w^{(i)} & \\text{ if } \\hat{y}_j^{(i)} = y^{(i)} \\\\\n",
    "    & w^{(i)} \\text{exp}\\bigl(\\alpha_j\\bigr) & \\text{ if } \\hat{y}_j^{(i)} \\neq y^{(i)} \\\\\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "- The weights are then normalized by dividing by $\\sum_{i=1}^m w^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps 1-3 are repeated until all predictors have been trained or a perfect predictor is established."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Equation 4: AdaBoost predictions*\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y}(\\mathbf{x}) = \\underset{k}{\\text{argmax}} \\sum_{\\substack{\\hat{y}_j(\\mathbf{x})=k\\\\j=1}}^N \\alpha_j\n",
    "\\end{equation*}\n",
    "\n",
    "- $N$ is the number of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                         class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=1,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort='deprecated',\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Use 200 decision stumps and the SAMME.R algorithm\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200,\n",
    "    algorithm='SAMME.R',\n",
    "    learning_rate=0.5\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, ada_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(ccp_alpha=0.0,\n",
       "                                                       criterion='mse',\n",
       "                                                       max_depth=1,\n",
       "                                                       max_features=None,\n",
       "                                                       max_leaf_nodes=None,\n",
       "                                                       min_impurity_decrease=0.0,\n",
       "                                                       min_impurity_split=None,\n",
       "                                                       min_samples_leaf=1,\n",
       "                                                       min_samples_split=2,\n",
       "                                                       min_weight_fraction_leaf=0.0,\n",
       "                                                       presort='deprecated',\n",
       "                                                       random_state=None,\n",
       "                                                       splitter='best'),\n",
       "                  learning_rate=0.5, loss='linear', n_estimators=200,\n",
       "                  random_state=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A regression variant also exists\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "ada_reg = AdaBoostRegressor(\n",
    "    DecisionTreeRegressor(max_depth=1),\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.5\n",
    ")\n",
    "ada_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15063502818008653"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, ada_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
