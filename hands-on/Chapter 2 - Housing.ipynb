{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:26:56.646127Z",
     "start_time": "2020-01-10T06:26:52.915220Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "\n",
    "DOWNLOAD_ROOT = 'https://raw.githubusercontent.com/ageron/handson-ml2/master/'\n",
    "HOUSING_PATH = os.path.join('datasets', 'housing')\n",
    "HOUSING_URL = f'{DOWNLOAD_ROOT}datasets/housing/housing.tgz'\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, 'housing.tgz')\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, 'housing.csv')\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:26:57.052984Z",
     "start_time": "2020-01-10T06:26:56.648125Z"
    }
   },
   "outputs": [],
   "source": [
    "fetch_housing_data()\n",
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:26:57.069910Z",
     "start_time": "2020-01-10T06:26:57.054949Z"
    }
   },
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:26:57.084871Z",
     "start_time": "2020-01-10T06:26:57.072900Z"
    }
   },
   "outputs": [],
   "source": [
    "housing['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:26:57.158704Z",
     "start_time": "2020-01-10T06:26:57.087862Z"
    }
   },
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:26:59.909590Z",
     "start_time": "2020-01-10T06:26:57.160666Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:26:59.919563Z",
     "start_time": "2020-01-10T06:26:59.911556Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_test(data, ratio):\n",
    "    # Set the random seed to ensure consistent shuffled indices\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Generate array of len(data) 'random' integers\n",
    "    shuffled_i = np.random.permutation(len(data))\n",
    "    \n",
    "    # Establish size of test set as a portion of len(data)\n",
    "    test_set_size = int(len(data) * ratio)\n",
    "    \n",
    "    # Test indices make up the one portion, training indeces make up the rest\n",
    "    test_i = shuffled_i[:test_set_size]\n",
    "    train_i = shuffled_i[test_set_size:]\n",
    "    \n",
    "    return data.iloc[train_i], data.iloc[test_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:26:59.933500Z",
     "start_time": "2020-01-10T06:26:59.922529Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set aside 20% of the data for a test set\n",
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "print(f'Train: {len(train_set)}\\nTest: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:26:59.942475Z",
     "start_time": "2020-01-10T06:26:59.936490Z"
    }
   },
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    \n",
    "    # Split values by ID checksum less than or equal to the ratio threshold\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:00.022340Z",
     "start_time": "2020-01-10T06:26:59.944468Z"
    }
   },
   "outputs": [],
   "source": [
    "# We choose to use an index column for ID as they are unique\n",
    "housing_with_id = housing.reset_index()\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:00.049187Z",
     "start_time": "2020-01-10T06:27:00.025263Z"
    }
   },
   "outputs": [],
   "source": [
    "#train_set.head()\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:00.122076Z",
     "start_time": "2020-01-10T06:27:00.053180Z"
    }
   },
   "outputs": [],
   "source": [
    "# We could also use lat and long to ensure unique values that don't rely on order\n",
    "housing_with_id['id'] = housing['longitude'] * 1000 + housing['latitude']\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:00.145930Z",
     "start_time": "2020-01-10T06:27:00.123987Z"
    }
   },
   "outputs": [],
   "source": [
    "#train_set.head()\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:00.851512Z",
     "start_time": "2020-01-10T06:27:00.148922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scikit-Learn has similar functions for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:00.872396Z",
     "start_time": "2020-01-10T06:27:00.853446Z"
    }
   },
   "outputs": [],
   "source": [
    "#train_set.head()\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:01.100784Z",
     "start_time": "2020-01-10T06:27:00.875391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Categorize incomes to ensure the test set represents the full set of data\n",
    "housing['income_cat'] = pd.cut(\n",
    "    housing['median_income'],\n",
    "    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "    labels=[1, 2, 3, 4, 5]\n",
    ")\n",
    "housing['income_cat'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:01.124753Z",
     "start_time": "2020-01-10T06:27:01.102780Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use Scikit-Learn's StratifiedShuffleSplit to split housing according to the income categories\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_i, test_i in split.split(housing, housing['income_cat']):\n",
    "    strat_train_set = housing.loc[train_i]\n",
    "    strat_test_set = housing.loc[test_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:01.149683Z",
     "start_time": "2020-01-10T06:27:01.126717Z"
    }
   },
   "outputs": [],
   "source": [
    "#strat_train_set.head()\n",
    "strat_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:01.162622Z",
     "start_time": "2020-01-10T06:27:01.151649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verify by checking the percentage of the test set that falls in each category\n",
    "strat_test_set['income_cat'].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:01.206505Z",
     "start_time": "2020-01-10T06:27:01.165612Z"
    }
   },
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    return data['income_cat'].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    'Overall': income_cat_proportions(housing),\n",
    "    'Stratified': income_cat_proportions(strat_test_set),\n",
    "    'Random': income_cat_proportions(test_set)\n",
    "}).sort_index()\n",
    "\n",
    "compare_props['Rand. %error'] = compare_props['Random'] / compare_props['Overall'] * 100 - 100\n",
    "compare_props['Strat. %error'] = compare_props['Stratified'] / compare_props['Overall'] * 100 - 100\n",
    "\n",
    "compare_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:01.217473Z",
     "start_time": "2020-01-10T06:27:01.209494Z"
    }
   },
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop('income_cat', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:01.240414Z",
     "start_time": "2020-01-10T06:27:01.220468Z"
    }
   },
   "outputs": [],
   "source": [
    "#strat_train_set.head()\n",
    "strat_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:01.253382Z",
     "start_time": "2020-01-10T06:27:01.246395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy the training set into an exploration set\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:01.603472Z",
     "start_time": "2020-01-10T06:27:01.259390Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the data by geographical location\n",
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:02.914944Z",
     "start_time": "2020-01-10T06:27:01.605436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add median house value to the visualization\n",
    "housing.plot(\n",
    "    kind='scatter', \n",
    "    x='longitude', \n",
    "    y='latitude', \n",
    "    alpha=0.4,\n",
    "    s=housing['population']/100,\n",
    "    label='population',\n",
    "    figsize=(10,7),\n",
    "    c='median_house_value',\n",
    "    cmap=plt.get_cmap('jet'),\n",
    "    colorbar=True\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:02.931869Z",
     "start_time": "2020-01-10T06:27:02.916938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the standard correlation coefficients (Pearson's r)\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:07.769380Z",
     "start_time": "2020-01-10T06:27:02.933864Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Select 4 most highly correlated attributes and generate a scatter matrix between them\n",
    "attributes = corr_matrix['median_house_value'].sort_values(ascending=False).keys()[0:4]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.228155Z",
     "start_time": "2020-01-10T06:27:07.771350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the median_income vs median_house_value plot; the horizontalish lines indicate potential anomalies\n",
    "housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.241093Z",
     "start_time": "2020-01-10T06:27:08.231121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create some additional attributes we may be interested in\n",
    "housing['rooms_per_household'] = housing['total_rooms']/housing['households']\n",
    "housing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms']\n",
    "housing['population_per_household'] = housing['population']/housing['households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.267021Z",
     "start_time": "2020-01-10T06:27:08.244085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get new correlations\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.276000Z",
     "start_time": "2020-01-10T06:27:08.269017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset to a clean copy of the data, separating predictors and labels for their respective transformations\n",
    "housing = strat_train_set.drop('median_house_value', axis=1)\n",
    "housing_labels = strat_train_set['median_house_value'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.284011Z",
     "start_time": "2020-01-10T06:27:08.278990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Address missing values in total_bedrooms\n",
    "\n",
    "# Option 1: Remove districts missing this attribute\n",
    "#housing.dropna(subset=['total_bedrooms'])\n",
    "\n",
    "# Option 2: Remove the entire column\n",
    "#housing.drop('total_bedrooms', axis=1)\n",
    "\n",
    "# Option 3: Fill missing values (median in this case)\n",
    "#median = housing['total_bedrooms'].median()\n",
    "#housing['total_bedrooms'].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.643727Z",
     "start_time": "2020-01-10T06:27:08.286974Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can use Scikit Learn to do the same for the entire dataset, but only if it's entirely numeric\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.652702Z",
     "start_time": "2020-01-10T06:27:08.645691Z"
    }
   },
   "outputs": [],
   "source": [
    "# Since ocean_proximity is the only non-numeric attribute, we make a copy of the data excluding only that\n",
    "housing_numeric = housing.drop('ocean_proximity', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.677608Z",
     "start_time": "2020-01-10T06:27:08.654667Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get rows missing total_bedrooms\n",
    "missing_total_bedrooms = housing_numeric[np.isnan(housing_numeric['total_bedrooms'])].index\n",
    "housing_numeric.loc[missing_total_bedrooms].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.718500Z",
     "start_time": "2020-01-10T06:27:08.679603Z"
    }
   },
   "outputs": [],
   "source": [
    "# The Imputer computes and stores the median of each attribute, useful if we encounter missing data later\n",
    "#imputer.fit(housing_numeric)\n",
    "\n",
    "# Apply the transformation with the Imputer to generate a NumPy array\n",
    "#X = imputer.transform(housing_numeric)\n",
    "X = imputer.fit_transform(housing_numeric)\n",
    "\n",
    "print(f'Imputer:\\n{imputer.statistics_}\\n\\nPandas:\\n{housing_numeric.median().values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.740438Z",
     "start_time": "2020-01-10T06:27:08.721492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the NumPy array back into a dataframe\n",
    "housing_transformed = pd.DataFrame(X, columns=housing_numeric.columns, index=housing_numeric.index)\n",
    "housing_transformed.loc[missing_total_bedrooms].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.755400Z",
     "start_time": "2020-01-10T06:27:08.742440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ocean_proximity was dropped from our numerical set, here it is added to its own dataframe\n",
    "housing_cat = housing[['ocean_proximity']]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.772351Z",
     "start_time": "2020-01-10T06:27:08.759390Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use Scikit Learn's Ordinal Encoder to encode ocean_proximity into numerical values\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.782328Z",
     "start_time": "2020-01-10T06:27:08.776341Z"
    }
   },
   "outputs": [],
   "source": [
    "# Access the enumerated categories by calling the hyperparameter categories_\n",
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.803270Z",
     "start_time": "2020-01-10T06:27:08.785320Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.814240Z",
     "start_time": "2020-01-10T06:27:08.806261Z"
    }
   },
   "outputs": [],
   "source": [
    "# Categories accessed the same as for ordinal_encoder\n",
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.844198Z",
     "start_time": "2020-01-10T06:27:08.816235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transformer class to add new attributes from earlier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "#housing_extra_attribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.893059Z",
     "start_time": "2020-01-10T06:27:08.846155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use Scikit Learn's Pipeline class to sequence the transformations\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the pipeline for numerical data\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_numeric)\n",
    "#housing_num_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.964861Z",
     "start_time": "2020-01-10T06:27:08.895025Z"
    }
   },
   "outputs": [],
   "source": [
    "# ColumnTransformer can apply different encoders to specified DataFrame columns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Separate the columns by numeric and categorical; indices would work as well\n",
    "num_attribs = list(housing_numeric)\n",
    "cat_attribs = ['ocean_proximity']\n",
    "#num_attribs = [housing.columns.get_loc(c) for c in list(housing_numeric) if c in housing]\n",
    "#cat_attribs = [housing.columns.get_loc('ocean_proximity')]\n",
    "\n",
    "# Use the numerical pipeline for the numerical attributes, one-hot for categorical\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "    ('cat', OneHotEncoder(), cat_attribs)\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "housing_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:08.986810Z",
     "start_time": "2020-01-10T06:27:08.966834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare a Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:09.003735Z",
     "start_time": "2020-01-10T06:27:08.988775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the model with a small sample of the data\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(f'Predictions: {lin_reg.predict(some_data_prepared)}')\n",
    "print(f'Labels: {list(some_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:09.018695Z",
     "start_time": "2020-01-10T06:27:09.007734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the accuracy using the full dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "\n",
    "print(f'MSE: {lin_mse}\\nRMSE: {lin_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:09.497443Z",
     "start_time": "2020-01-10T06:27:09.020688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try a Decision Tree to improve the model accuracy\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:09.518360Z",
     "start_time": "2020-01-10T06:27:09.499413Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the accuracy of the new model\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "\n",
    "print(f'MSE: {tree_mse}\\nRMSE: {tree_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:12.677941Z",
     "start_time": "2020-01-10T06:27:09.520353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Validate the model using Scikit Learn's K-fold cross-validation feature\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# cross_val_score expects a utility function (as opposed to a cost function), hence neg_mean_squared_error\n",
    "scores = cross_val_score(\n",
    "    tree_reg,\n",
    "    housing_prepared,\n",
    "    housing_labels,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:12.687915Z",
     "start_time": "2020-01-10T06:27:12.679909Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function for printing scores\n",
    "def display_scores(scores):\n",
    "    print(f'Scores: {scores}\\nMean: {scores.mean()}\\nStandard Deviation: {scores.std()}')\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:12.861229Z",
     "start_time": "2020-01-10T06:27:12.692873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-validate the Linear Regression model\n",
    "lin_scores = cross_val_score(\n",
    "    lin_reg,\n",
    "    housing_prepared,\n",
    "    housing_labels,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:12.868182Z",
     "start_time": "2020-01-10T06:27:12.863198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try one more model (Random Forest Regressor)\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#forest_reg = RandomForestRegressor()\n",
    "#forest_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:12.878155Z",
     "start_time": "2020-01-10T06:27:12.871175Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Validate once more\n",
    "forest_scores = cross_val_score(\n",
    "    forest_reg,\n",
    "    housing_prepared,\n",
    "    housing_labels,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:12.885142Z",
     "start_time": "2020-01-10T06:27:12.880150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use grid search to tune hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:28:19.905186Z",
     "start_time": "2020-01-10T06:28:19.890227Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators': [30, 100, 300], 'max_features': [.25, .5, .75, 1.0]},\n",
    "    {'bootstrap': [False], 'n_estimators': [30, 100], 'max_features': [.5, .75, 1.0]}\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(\n",
    "    forest_reg,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:58.630680Z",
     "start_time": "2020-01-10T06:27:58.617685Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:59.653603Z",
     "start_time": "2020-01-10T06:27:59.641661Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.065597Z",
     "start_time": "2020-01-10T06:26:53.099Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_results = grid_search.cv_results_\n",
    "for mean_score, params in zip(cv_results['mean_test_score'], cv_results['params']):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.067595Z",
     "start_time": "2020-01-10T06:26:53.102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determine and display importance of each feature\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_room']\n",
    "cat_encoder = full_pipeline.named_transformers_['cat']\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.070586Z",
     "start_time": "2020-01-10T06:26:53.106Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model performance with the test set\n",
    "final_model =  grid_search.best_estimator_\n",
    "X_test = strat_test_set.drop('median_house_value', axis=1)\n",
    "y_test = strat_test_set['median_house_value'].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.071584Z",
     "start_time": "2020-01-10T06:26:53.108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the 95% confidence interval of the results\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.074573Z",
     "start_time": "2020-01-10T06:26:53.111Z"
    }
   },
   "outputs": [],
   "source": [
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test) ** 2\n",
    "np.sqrt(\n",
    "    stats.t.interval(\n",
    "        confidence,\n",
    "        len(squared_errors) - 1,\n",
    "        loc=squared_errors.mean(),\n",
    "        scale=stats.sem(squared_errors)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Try a Support Vector Machine regressor (sklearn.svm.SVR), with various hyperparameters such as kernel=\"linear\" (with various values for the C hyperparameter) or kernel=\"rbf\" (with various values for the C and gamma hyperparameters). Don't worry about what these hyperparameters mean for now. How does the best SVR predictor perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.075573Z",
     "start_time": "2020-01-10T06:26:53.114Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function for cross-validation of various models\n",
    "def cross_validate(model):\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        housing_prepared,\n",
    "        housing_labels,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=10\n",
    "    )\n",
    "    rmse_scores = np.sqrt(-scores)\n",
    "    display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.077567Z",
     "start_time": "2020-01-10T06:26:53.117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.079563Z",
     "start_time": "2020-01-10T06:26:53.119Z"
    }
   },
   "outputs": [],
   "source": [
    "# Linear kernel\n",
    "#linear_svm = SVR(kernel='linear')\n",
    "#linear_svm.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.080560Z",
     "start_time": "2020-01-10T06:26:53.122Z"
    }
   },
   "outputs": [],
   "source": [
    "#cross_validate(linear_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.082554Z",
     "start_time": "2020-01-10T06:26:53.124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gaussian kernel\n",
    "#rbf_svm = SVR(kernel='rbf')\n",
    "#rbf_svm.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.084576Z",
     "start_time": "2020-01-10T06:26:53.127Z"
    }
   },
   "outputs": [],
   "source": [
    "#cross_validate(rbf_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.085547Z",
     "start_time": "2020-01-10T06:26:53.130Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "svm_params = [\n",
    "    {\n",
    "        'kernel': ['linear'], \n",
    "        'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.]\n",
    "    },{\n",
    "        'kernel': ['rbf'], \n",
    "        'C': [10., 30., 100., 300., 1000., 3000.],\n",
    "        'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]\n",
    "    }\n",
    "]\n",
    "\n",
    "svm_reg = SVR()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    svm_reg,\n",
    "    svm_params,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.087545Z",
     "start_time": "2020-01-10T06:26:53.132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to get RMSE using best hyperparameters from search results\n",
    "def test_best_hyperparameters(search_results):\n",
    "    # Evaluate the model performance with the test set\n",
    "    final_model =  search_results.best_estimator_\n",
    "    X_test = strat_test_set.drop('median_house_value', axis=1)\n",
    "    y_test = strat_test_set['median_house_value'].copy()\n",
    "\n",
    "    X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "    final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "    squared_errors = (final_predictions - y_test) ** 2\n",
    "    rmse = np.sqrt(\n",
    "        stats.t.interval(\n",
    "            0.95,\n",
    "            len(squared_errors) - 1,\n",
    "            loc=squared_errors.mean(),\n",
    "            scale=stats.sem(squared_errors)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f'Best hyperparameters: {search_results.best_params_}\\nRMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.088538Z",
     "start_time": "2020-01-10T06:26:53.135Z"
    }
   },
   "outputs": [],
   "source": [
    "test_best_hyperparameters(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Try replacing GridSearchCV with RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.090535Z",
     "start_time": "2020-01-10T06:26:53.155Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.092529Z",
     "start_time": "2020-01-10T06:26:53.157Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param_distribs = [\n",
    "    {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': reciprocal(20, 200000),\n",
    "        'gamma': expon(scale=1.0)\n",
    "    }\n",
    "]\n",
    "\n",
    "svm_reg = SVR()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    svm_reg,\n",
    "    param_distributions=param_distribs,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(housing_prepared, housing_labels)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:27:13.094523Z",
     "start_time": "2020-01-10T06:26:53.160Z"
    }
   },
   "outputs": [],
   "source": [
    "test_best_hyperparameters(random_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Try adding a transformer in the preparation pipeline to select only the most important attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:28:48.063334Z",
     "start_time": "2020-01-10T06:28:48.058347Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:28:48.295621Z",
     "start_time": "2020-01-10T06:28:48.281656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['median_income', 'INLAND', 'pop_per_hhold', 'longitude',\n",
       "       'latitude'], dtype='<U18')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall feature_importances calculated with the RandomForestRegressor grid search\n",
    "feature_importances = [\n",
    "    (0.3758106238507585, 'median_income'),\n",
    "    (0.1531916154269434, 'INLAND'),\n",
    "    (0.11316103307394935, 'pop_per_hhold'),\n",
    "    (0.07380408048739615, 'longitude'),\n",
    "    (0.06846831115043858, 'latitude'),\n",
    "    (0.053333614007285805, 'rooms_per_hhold'),\n",
    "    (0.050694287855152495, 'bedrooms_per_room'),\n",
    "    (0.04175094704179171, 'housing_median_age'),\n",
    "    (0.014694621081341083, 'total_rooms'),\n",
    "    (0.014257664082623828, 'population'),\n",
    "    (0.014015238382847444, 'total_bedrooms'),\n",
    "    (0.013770060742181808, 'households'),\n",
    "    (0.0069693152264157215, '<1H OCEAN'),\n",
    "    (0.004249300120337744, 'NEAR OCEAN'),\n",
    "    (0.0017753375854791455, 'NEAR BAY'),\n",
    "    (5.3949885057266795e-05, 'ISLAND')\n",
    "]\n",
    "\n",
    "# Split feature importances into two lists \n",
    "importances, attributes = zip(*feature_importances)\n",
    "\n",
    "# Get sorted indices of top k elements in a list\n",
    "def indices_of_top_k(a, k):\n",
    "    b = np.argpartition(np.array(a), -k)[-k:]\n",
    "    return np.sort(b)\n",
    "\n",
    "top_k = indices_of_top_k(importances, 5)\n",
    "np.array(attributes)[top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:28:48.762540Z",
     "start_time": "2020-01-10T06:28:48.754589Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new transformer to select only the indices determined by indices_of_top_k\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ImportantAttributeSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, importances, k):\n",
    "        self.importances = importances\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.indices_ = indices_of_top_k(self.importances, self.k)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.indices_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:30:04.029835Z",
     "start_time": "2020-01-10T06:30:04.026840Z"
    }
   },
   "outputs": [],
   "source": [
    "top_attributes_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('top_attributes', ImportantAttributeSelector(importances, 5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:30:48.603486Z",
     "start_time": "2020-01-10T06:30:48.554587Z"
    }
   },
   "outputs": [],
   "source": [
    "housing_prepared_top_k = top_attributes_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:31:44.570265Z",
     "start_time": "2020-01-10T06:31:44.564281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15604281,  0.77194962,  0.74333089, -0.49323393, -0.44543821],\n",
       "       [-1.17602483,  0.6596948 , -1.1653172 , -0.90896655, -1.0369278 ],\n",
       "       [ 1.18684903, -1.34218285,  0.18664186, -0.31365989, -0.15334458],\n",
       "       [-0.01706767,  0.31357576, -0.29052016, -0.36276217, -0.39675594],\n",
       "       [ 0.49247384, -0.65929936, -0.92673619,  1.85619316,  2.41221109]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_prepared_top_k[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T06:31:45.090088Z",
     "start_time": "2020-01-10T06:31:45.082110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15604281,  0.77194962,  0.74333089, -0.49323393, -0.44543821],\n",
       "       [-1.17602483,  0.6596948 , -1.1653172 , -0.90896655, -1.0369278 ],\n",
       "       [ 1.18684903, -1.34218285,  0.18664186, -0.31365989, -0.15334458],\n",
       "       [-0.01706767,  0.31357576, -0.29052016, -0.36276217, -0.39675594],\n",
       "       [ 0.49247384, -0.65929936, -0.92673619,  1.85619316,  2.41221109]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_prepared[0:5, top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Try creating a single pipeline that does the full data preparation plus the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('top_attributes', ImportantAttributeSelector(importances, 5)),\n",
    "    ('svm_reg', SVR(**grid_search.best_estimator_)) # Best results from Exercise 1\n",
    "])\n",
    "\n",
    "prediction_pipeline.fit_transform(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prediction_pipeline.predict(housing[:5])\n",
    "labels = housing_labels.iloc[:5]\n",
    "\n",
    "print(f'Predictions: {data}\\nLabels: {labels}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Automatically explore some preparation options using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "        'feature_selection__k': list(range(1, len(feature_importances) + 1))\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search_prep = GridSearchCV(\n",
    "    prepare_select_and_predict_pipeline, \n",
    "    param_grid, \n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error', \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search_prep.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_prep.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
